{"cells":[{"cell_type":"markdown","source":["## üè† **Las ventajas de jugar en casa**\n","---\n","\n","### ‚öôÔ∏è Native Executive Engine\n","\n","Motor de ejecuci√≥n vectorizado que sustituye la ejecuci√≥n tradicional de Java por c√≥digo nativo en C++ para maximizar el rendimiento.\n","\n","Su funci√≥n principal es procesar las tareas de Spark directamente en el hardware moderno, eliminando la sobrecarga de la JVM y la serializaci√≥n para acelerar dr√°sticamente las consultas y transformaciones de datos.\n","\n","En escenarios compatibles, se habla de mejoras de un 2x-5x."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d34761e-447f-43b3-89c1-70956b1e443c"},{"cell_type":"code","source":["spark.conf.set(\"spark.native.enabled\", \"true\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11ec4618-3157-4b5f-91e8-027986da025d"},{"cell_type":"markdown","source":["### üõ†Ô∏è Optimizando, que es gerundio\n","\n","Disponemos de diferentes propiedades que facilitan el ajuste y tunning de operaciones de Spark de forma autom√°tica.\n","\n","Microsoft recomienda activar algunas de ellas, pero esto no significa que sea activarlas y ya.\n","\n","#### ‚ö°Fast optimize\n","\n","Analiza de forma inteligente los archivos de una tabla Delta y omite las operaciones de compactaci√≥n que no mejoren el rendimiento de forma significativa cuando ejecutamos un comando OPTIMIZE expl√≠cito.\n","\n","En lugar de compactar archivos ciegamente cada vez que existen archivos peque√±os, Fast Optimize eval√∫a si cada grupo de archivos candidato cumple los objetivos de compactaci√≥n recomendados. De lo contrario, omite ese grupo o reduce el n√∫mero de archivos que compacta."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fa34815-c77a-405a-8e26-038b75a4b46b"},{"cell_type":"code","source":["spark.conf.set('spark.microsoft.delta.optimize.fast.enabled', True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4984cb7-eb6e-4c80-8162-c1fc31d99be2"},{"cell_type":"markdown","source":["#### üóúÔ∏è File-level compaction targets\n","\n","Para evitar la recompactaci√≥n de archivos ya compactados. \n","\n","Cuando se habilita, los archivos no se recompactan si anteriormente cumpl√≠an al menos la mitad del tama√±o objetivo del archivo en el momento que se compactaron. Especialmente √∫til ya que el tama√±o objetivo de compactaci√≥n puede cambiar con el tiempo."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a4c8c24-d131-4ec9-a3b0-1d06a340bbd9"},{"cell_type":"code","source":["spark.conf.set('spark.microsoft.delta.optimize.fileLevelTarget.enabled', True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a960b70-16e5-4ca9-a623-a60d78761597"},{"cell_type":"markdown","source":["## üì¶ **Vienen ya en la caja**\n","---\n","\n","Propiedades inherentes de Spark que tambi√©n se recomienda activar/tener en cuenta.\n","\n","Pueden activarse tanto a nivel de sesi√≥n como a nivel de tabla.\n","\n","#### üÖ∞Ô∏è Auto compaction\n","Eval√∫a el estado despu√©s de cada operaci√≥n de escritura. \n","\n","Cuando detecta demasiados archivos peque√±os, ejecuta una operaci√≥n OPTIMIZE inmediatamente despu√©s de que se confirme la escritura. Este enfoque es √≥ptimo porque la compactaci√≥n solo se ejecuta cuando se determina que es beneficioso. Microsoft recomienda activar este ajuste frente a trabajos programados de mantenimiento (OPTIMIZE), ahora esto siempre que la latencia adicional que supone sea asumible por el proceso relacionado.\n","\n","Resulta muy beneficioso usarlo en conjunci√≥n con la siguiente.\n","\n","**OJO** No elimina la necesidad de uso del comando OPTIMIZE en todos los escenarios."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"050b728b-55e4-4233-a5b5-83299a3b39c4"},{"cell_type":"code","source":["spark.conf.set('spark.databricks.delta.autoCompact.enabled', True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5228fe45-a02e-48f3-81d0-4bc45510469a"},{"cell_type":"markdown","source":["#### ‚úçüèª Optimize write\n","\n","Reduce la sobrecarga de los archivos peque√±os mediante la compactaci√≥n previa a la escritura, lo que genera menos archivos y de mayor tama√±o. Este enfoque mezcla los datos en memoria antes de que Spark escriba, lo que maximiza el potencial de generar archivos de tama√±o adecuado sin necesidad de operaciones de compactaci√≥n tras la escritura.\n","\n","No obstante, debe utilizarse con precauci√≥n, ya que su coste computacional puede a√±adir un tiempo de procesamiento excesivo e innecesario. Principalmente es beneficiosa cuando una operaci√≥n de escritura producir√≠a archivos peque√±os que ser√≠an candidatos para una compactaci√≥n posterior.\n","\n","Muy adecuada para:\n","- Tablas particionadas\n","- Tablas con inserciones peque√±as frecuentes\n","- Operaciones que probablemente afecten a muchos archivos (MERGE, UPDATE y DELETE)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27157270-8a22-4303-8235-fd082f9bbecf"},{"cell_type":"code","source":["spark.conf.set('spark.databricks.delta.optimizeWrite.enabled', True)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4054b1ff-b519-4464-8b01-7b4c35c44063"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"es"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}